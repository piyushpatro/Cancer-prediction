# -*- coding: utf-8 -*-
"""Skill geek cancer dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KFdGpfpj5CdO09VSrElXJt7m3nf098jq
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import davies_bouldin_score, accuracy_score, confusion_matrix
# %matplotlib inline

df=pd.read_csv('/content/drive/MyDrive/data.csv')
df.head()

df.info()

y=df['diagnosis']
X=df.drop(labels=['Unnamed: 32', 'id', 'diagnosis'], axis=1)

#1.1 Standardizing the training data(X)

scaler=StandardScaler()
X=scaler.fit_transform(X)
print(X)

#1.2 finding the principle components of X

pca=PCA(n_components=10)
components=pca.fit_transform(X)
print('Components: \n', pca.components_)

# calculating the corresponding loads
loadings=pca.components_.T * np.sqrt(pca.explained_variance_)         # loadings=eigenvectors*(eigenvalues)^0.5
print('loadings: \n', loadings)

# Plotting eigen values as a function of its number
fig, ax=plt.subplots(1, 1)
ax.plot(pca.explained_variance_, marker='o')
ax.set_xlabel('PCA number')
ax.set_ylabel('Eigen values')
plt.show()

#1.3 Kaiser rule: the eigen value should be greater than 1
n=0
for i in pca.explained_variance_:
  if(i>1):
    n=n+1
  else:
    break
print('The number of major components that should be retained as per Kaiser rule is:',n)

# Conditional number rule: max(eigen values)/(eigen value)<10
x=0
max=np.max(pca.explained_variance_)
for i in pca.explained_variance_:
  if(max/i<10):
    x=x+1
  else:
    break
print('The number of major components that should be retained as per Conditional number rule is:',x)

#2.1 Data visualisation using histograms
sns.histplot(components[:, 0], kde=True)
plt.show()
sns.histplot(components[:, 1], kde=True)
plt.show()
sns.histplot(components[:, 2], kde=True)
plt.show()

#2.1 Data visualisation using histograms
sns.barplot(x=np.arange(1, len(pca.explained_variance_)+1, 1) ,y=pca.explained_variance_)
plt.xlabel('component number')
plt.ylabel('Variance')
plt.show()

# As the variance of PCA_1 is highest, therefore it is best component for seperation of classes

#2.2 Data visualisation using PCA
sns.scatterplot(x=components[:, 0], y=components[:, 1], hue=y)
plt.show()

sns.scatterplot(x=components[:, 0], y=components[:, 2], hue=y)
plt.show()

sns.scatterplot(x=components[:, 1], y=components[:, 2], hue=y)
plt.show()

#3.1 K means clustering of data
k_means_2=KMeans(n_clusters=2, n_init=100)
k_means_2.fit(components[:, 0:2])
pred_2=k_means_2.predict(components[:,0:2])
score_2=davies_bouldin_score(components[:, 0:2], k_means_2.predict(components[:,0:2]))
cluster_centers_2=k_means_2.cluster_centers_

k_means_3=KMeans(n_clusters=3, n_init=100)
k_means_3.fit(components[:, 0:2])
pred_3=k_means_3.predict(components[:,0:2])
score_3=davies_bouldin_score(components[:, 0:2], k_means_3.predict(components[:,0:2]))
cluster_centers_3=k_means_3.cluster_centers_


k_means_5=KMeans(n_clusters=5, n_init=100)
k_means_5.fit(components[:, 0:2])
pred_5=k_means_5.predict(components[:,0:2])
score_5=davies_bouldin_score(components[:, 0:2], k_means_5.predict(components[:,0:2]))
cluster_centers_5=k_means_5.cluster_centers_

print('Scores are for 2, 3, 5: \n', score_2, score_3, score_5)
print('Cluster centroids are for 2, 3, 5: \n', cluster_centers_2, cluster_centers_3, cluster_centers_5)

report=pd.DataFrame([[cluster_centers_2[0], score_2, 2],
              [cluster_centers_2[1], score_2, 2],
              [cluster_centers_3[0], score_3, 3],
              [cluster_centers_3[1], score_3, 3],
              [cluster_centers_3[2], score_3, 3],
              [cluster_centers_5[0], score_5, 5],
              [cluster_centers_5[1], score_5, 5],
              [cluster_centers_5[2], score_5, 5],
              [cluster_centers_5[3], score_5, 5],
              [cluster_centers_5[4], score_5, 5]], columns=['Centroids (x,y)', 'Score', 'Clusters'])
#Report
display(report)

#3.2 Visualisation

fig, ax=plt.subplots(1,1)
ax.scatter(components[:, 0], components[:,1], c=pred_2, alpha=0.6)
ax.scatter(cluster_centers_2[:, 0], cluster_centers_2[:, 1], marker='x', s=100)
plt.show()

fig, ax=plt.subplots(1,1)
ax.scatter(components[:, 0], components[:,1], c=pred_3, alpha=0.4)
ax.scatter(cluster_centers_3[:, 0], cluster_centers_3[:, 1], marker='x', s=100)
plt.show()

fig, ax=plt.subplots(1,1)
ax.scatter(components[:, 0], components[:,1], c=pred_5, alpha=0.4)
ax.scatter(cluster_centers_5[:, 0], cluster_centers_5[:, 1], marker='x', s=100)
plt.show()

#4.1 calculation of purity of cluster
y=np.array(pd.get_dummies(y, drop_first=True))
y=y.reshape(569)
cm_2=confusion_matrix(pred_2, y)
cm_3=confusion_matrix(pred_3, y)
cm_5=confusion_matrix(pred_5, y)

purity_2=(np.max(cm_2[0,:])+np.max(cm_2[1,:]))/np.sum(cm_2)
purity_3=(np.max(cm_3[0,:])+np.max(cm_3[1,:]+np.max(cm_3[2,:])))/np.sum(cm_3)
purity_5=(np.max(cm_5[0,:])+np.max(cm_5[1,:]+np.max(cm_5[2,:]))+np.max(cm_5[3,:])+np.max(cm_5[4,:]))/np.sum(cm_5)

print('purity score for 2, 3, 5 is:', purity_2, purity_3, purity_5)

#4.2 Relative Information gain

def InformationGain(class0, class1):
	return -(class0 * np.log2(class0) + class1 * np.log2(class1))

_, counts_pred=np.unique(pred_2, return_counts=True)
_, counts_y=np.unique(y, return_counts=True)

class_1_pred=counts_pred[0]/np.sum(counts_pred)
class_2_pred=counts_pred[1]/np.sum(counts_pred)
pred_gain=InformationGain(class_1_pred, class_2_pred)

class_1_y=counts_y[0]/np.sum(counts_y)
class_2_y=counts_y[1]/np.sum(counts_y)
y_gain=InformationGain(class_1_y, class_2_y)

relative_gain=np.round(y_gain/pred_gain - 1, 3)
print('Relative gain is:',relative_gain)

# For 2 clusters
centroid_0=components[58]
centroid_1=components[0]

D=np.sqrt(np.sum((centroid_0-components)**2, axis=1)), np.sqrt(np.sum((centroid_1-components)**2, axis=1))
D=np.array(D).T
C0=[]
C1=[]
for i in range(D.shape[0]):
  if(D[i, 0]>=D[i, 1]):
    C0.append(components[i])
  else:
    C1.append(components[i])
C0=np.array(C0)
C1=np.array(C1)
centroid_0=(np.max(C0, axis=0)+np.min(C0, axis=0)/2)
centroid_1=(np.max(C1, axis=0)+np.min(C1, axis=0)/2)

D

centroid_2

a=np.array(y)
np.unique(a, return_counts=True)

def euclidean_distance(x1, x2):
    return np.sqrt(np.sum((x1 - x2)**2))

class KMeans():

    def __init__(self, K=5, max_iters=100):
        self.K = K
        self.max_iters = max_iters
        self.clusters = [[] for _ in range(self.K)]
        self.centroids = []

    def predict(self, X):
        self.X = X
        self.n_samples, self.n_features = X.shape
        random_sample_idxs = np.random.choice(self.n_samples, self.K, replace=False)
        self.centroids = [self.X[idx] for idx in random_sample_idxs]
        for _ in range(self.max_iters):
            self.clusters = self._create_clusters(self.centroids)
            centroids_old = self.centroids
            self.centroids = self._get_centroids(self.clusters)
            if self._is_converged(centroids_old, self.centroids):
              break
        predictions=self._get_cluster_labels(self.clusters)
        return predictions

    def _get_cluster_labels(self, clusters):
        labels = np.empty(self.n_samples)

        for cluster_idx, cluster in enumerate(clusters):
            for sample_index in cluster:
                labels[sample_index] = cluster_idx
        return labels

    def _create_clusters(self, centroids):
        clusters = [[] for _ in range(self.K)]
        for idx, sample in enumerate(self.X):
            centroid_idx = self._closest_centroid(sample, centroids)
            clusters[centroid_idx].append(idx)
        return clusters

    def _closest_centroid(self, sample, centroids):
        distances = [euclidean_distance(sample, point) for point in centroids]
        closest_index = np.argmin(distances)
        return closest_index

    def _get_centroids(self, clusters):
        centroids = np.zeros((self.K, self.n_features))
        for cluster_idx, cluster in enumerate(clusters):
            cluster_mean = np.mean(self.X[cluster], axis=0)
            centroids[cluster_idx] = cluster_mean
        return centroids

    def _is_converged(self, centroids_old, centroids):
        distances = [euclidean_distance(centroids_old[i], centroids[i]) for i in range(self.K)]
        return sum(distances) == 0

kmeans=KMeans(K=2, max_iters=100)

davies_bouldin_score(X, kmeans.predict(X))